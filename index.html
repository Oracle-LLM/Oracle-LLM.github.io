<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="OracleLLM">
  <meta name="keywords" content="OracleLLM">
  <meta name="viewport" content="width=device-width, user-scalable=yes, initial-scale=0.3, maxmum-scale=1.0, minimum-scale=0.3">
  <style>
    body {
      display: flex;
      flex-direction: column;
    }

    
    .dot {
            height: 10px;
            width: 10px;
            background-color: black;
            border-radius: 50%;
            display: inline-block;
          }
    .column {
            float: left;
            width: 33.33%;
            padding: 5px;
          }
    .column2 {
        float: left;
        width: 48%;
        padding: 5px;
      }
.imageo{
  margin-top: 20px;
  width: 200px;
  height: 200px;
  border-radius:200px;
}
/* 清除图像容器后的浮动 */
.row::after {
  content: "";
  clear: both;
  display: table;
}
  </style>
  <title>OracleLLM: Empowering LLM with Self-Feedback</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">
  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <!-- (A) LOAD CSS & JS -->
  <link rel="stylesheet" href="collapse-list.css">
  <link rel="stylesheet" type="text/css" href="style.css">
  <script src="collapse-list.js"></script>


  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <img src="static/images/logo.png" alt="OracleLLM: Empowering LLM with Self-Feedback" width="800"/>
          <!-- <h1 class="title is-1 publication-title">OracleLLM: Empowering AI with<br>Self-Feedback</h1> -->
        </div>
      </div>
    </div>
  </div>
</section>



<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">About</h2>
        <div class="content has-text-justified">
          <p>
            <span>OracleLLM is an open-source research organization dedicated to exploring and advancing the concept of <strong>LLMs-as-Oracles</strong>. LLMs-as-Oracle refers to replacing human-generated golden references with the output and feedback produced by large language models (LLMs), treating them as authoritative benchmarks and learning signals for various tasks and applications. Our research areas include:</span>
            <span>(1). Data synthesis and annotation with LLMs.</span>
            <span>(2). LLMs as evaluators or judges for various tasks.</span>
            <span>(3). Employing LLMs' self-feedback as guidance for planning, self-improving and decision-making.</span>
            <span>Check more details on our current progress including papers, projects, and reading list!</span>
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Papers</h2>
        <div class="content has-text-justified">

          <h4 class="title is-4">Survey & Benchmark</h4>
          <ul type='disc'> 
            <li>
              <a href="https://arxiv.org/pdf/2402.13446">Large Language Models for Data Annotation: A Survey.</a><br> Zhen Tan*, Dawei Li*, Song Wang*, Alimohammad Beigi, Bohan Jiang, Amrita Bhattacharjee, Mansooreh Karami, Jundong Li, Lu Cheng, Huan Liu;<br><b>EMNLP 2024 Under Review</b>
              <details>
                <summary> Details </summary>
                <ul type='disc'>
                      <li> Data annotation is the labeling or tagging of raw data with relevant information, essential for improving the efficacy of machine learning models. The process, however, is labor-intensive and expensive. The emergence of advanced Large Language Models (LLMs), exemplified by GPT-4, presents an unprecedented opportunity to revolutionize and automate the intricate process of data annotation. While existing surveys have extensively covered LLM architecture, training, and general applications, this paper uniquely focuses on their specific utility for data annotation. This survey contributes to three core aspects: LLM-Based Data Annotation, Assessing LLM-generated Annotations, and Learning with LLM-generated annotations. Furthermore, the paper includes an in-depth taxonomy of methodologies employing LLMs for data annotation, a comprehensive review of learning strategies for models incorporating LLM-generated annotations, and a detailed discussion on primary challenges and limitations associated with using LLMs for data annotation. As a key guide, this survey aims to direct researchers and practitioners in exploring the potential of the latest LLMs for data annotation, fostering future advancements in this critical domain. </li>
                </ul>
              </details>
            </li>

                <li>
                  <a href="">A Survey for LLM-as-a-Judge</a><br><b>Comming Soon!</b>
                </li>
          </ul>

          <h4 class="title is-4">Data Synthesis</h4>
          <ul type='disc'>
            <li>
              <a href="https://arxiv.org/pdf/2405.04819">DALK:Dynamic Co-Augmentation of LLMs and KG to Answer Alzheimer's Disease Questions with Scientific Literature.</a><br> Dawei Li, Shu Yang, Zhen Tan, Jae Young Baik, Sunkwon Yun, Joseph Lee, Aaron Chacko, Bojian Hou, Duy Duong-Tran, Ying Ding, Huan Liu, Li Shen, Tianlong Chen;<br><b>EMNLP 2024 Under Review</b>
              <details>
                <summary> Details </summary>
                <ul type='disc'>
                      <li> Recent advancements in large language models (LLMs) have achieved promising performances across various applications. Nonetheless, the ongoing challenge of integrating long-tail knowledge continues to impede the seamless adoption of LLMs in specialized domains. In this work, we introduce DALK, a.k.a. Dynamic Co-Augmentation of LLMs and KG, to address this limitation and demonstrate its ability on studying Alzheimer's Disease (AD), a specialized sub-field in biomedicine and a global health priority. With a synergized framework of LLM and KG mutually enhancing each other, we first leverage LLM to construct an evolving AD-specific knowledge graph (KG) sourced from AD-related scientific literature, and then we utilize a coarse-to-fine sampling method with a novel self-aware knowledge retrieval approach to select appropriate knowledge from the KG to augment LLM inference capabilities. The experimental results, conducted on our constructed AD question answering (ADQA) benchmark, underscore the efficacy of DALK. Additionally, we perform a series of detailed analyses that can offer valuable insights and guidelines for the emerging topic of mutually enhancing KG and LLM. </li>
                </ul>
            </details>
            </li> 
            <li>
              <a href="https://arxiv.org/pdf/2402.01729">Contextualization Distillation from Large Language Model for Knowledge Graph Completion.</a><br>Dawei Li, Zhen Tan, Tianlong Chen, Huan Liu;<br><b>EACL 2024</b>
              <details>
                <summary> Details </summary>
                <ul type='disc'>
                      <li> While textual information significantly enhances the performance of pre-trained language models (PLMs) in knowledge graph completion (KGC), the static and noisy nature of existing corpora collected from Wikipedia articles or synsets definitions often limits the potential of PLM-based KGC models. To surmount these challenges, we introduce the Contextualization Distillation strategy, a versatile plug-in-and-play approach compatible with both discriminative and generative KGC frameworks. Our method begins by instructing large language models (LLMs) to transform compact, structural triplets into context-rich segments. Subsequently, we introduce two tailored auxiliary tasks, reconstruction and contextualization, allowing smaller KGC models to assimilate insights from these enriched triplets. Comprehensive evaluations across diverse datasets and KGC techniques highlight the efficacy and adaptability of our approach, revealing consistent performance enhancements irrespective of underlying pipelines or architectures. Moreover, our analysis makes our method more explainable and provides insight into generating path selection, as well as the choosing of suitable distillation tasks. </li>
                </ul>
            </details>
            </li>
          </ul>

          
          <h4 class="title is-4">LLM-as-a-Judge</h4>
          <ul type='disc'> 
            <li>
                <a href="https://arxiv.org/pdf/2405.04086">Optimizing Language Model's Reasoning Abilities with Weak Supervision</a><br>Yongqi Tong, Sizhe Wang, Dawei Li, Yifan Wang, Simeng Han, Zi Lin, Chengsong Huang, Jiaxin Huang, Jingbo Shang;<br><b>EMNLP 2024 Under Review</b>
                <details>
                  <summary> Details </summary>
                  <ul type='disc'>
                        <li> While Large Language Models (LLMs) have demonstrated proficiency in handling complex reasoning, much of the past work has depended on extensively annotated datasets by human experts. However, this reliance on fully-supervised annotations poses scalability challenges, particularly as models and data requirements grow. In this work, we begin by analyzing the limitations of existing data-efficient reinforcement learning (RL) methods in LLMs' reasoning enhancement. To mitigate this, we introduce self-reinforcement, an efficient weak-to-strong approach to optimize language models' reasoning abilities utilizing both annotated and unlabeled samples. Our method enhances the quality of synthetic feedback by fully harnessing annotated seed data and introducing a novel self-filtering mechanism to remove invalid pairs. We also present PuzzleBen, a weakly supervised benchmark for reasoning that comprises 25,147 complex questions, answers, and human-generated rationales across various domains, such as brainteasers, puzzles, riddles, parajumbles, and critical reasoning tasks. Our experiments underscore the significance of PuzzleBen, as well as the effectiveness of our methodology as a promising direction in future endeavors. </li>
                  </ul>
              </details>
              </li>      
           </ul>


           <h4 class="title is-4">Self-Feedback</h4>
          <ul type='disc'> 
              <li>
                <a href="https://arxiv.org/pdf/2403.20046">Can LLMs Learn from Previous Mistakes? Investigating LLMs' Errors to Boost for Reasoning</a><br> Yongqi Tong, Dawei Li, Sizhe Wang, Yujia Wang, Fei Teng, Jingbo Shang;<br> <b>ACL 2024</b>
                <details>
                    <summary> Details </summary>
                    <ul type='disc'>
                          <li> Recent works have shown the benefits to LLMs from fine-tuning golden-standard Chain-of-Thought (CoT) rationales or using them as correct examples in few-shot prompting. While humans can indeed imitate correct examples, learning from our mistakes is another vital aspect of human cognition. Hence, a question naturally arises: \textit{can LLMs learn and benefit from their mistakes, especially for their reasoning? } This study investigates this problem from both the prompting and model-tuning perspectives. We begin by introducing \textsc{CoTErrorSet}, a new benchmark with 609,432 questions, each designed with both correct and error references, and demonstrating the types and reasons for making such mistakes. To explore the effectiveness of those mistakes, we design two methods: (1) \textbf{Self-rethinking} prompting guides LLMs to rethink whether they have made similar previous mistakes; and (2) \textbf{Mistake tuning} involves finetuning models in both correct and incorrect reasoning domains, rather than only tuning models to learn ground truth in traditional methodology. We conduct a series of experiments to prove LLMs can obtain benefits from mistakes in both directions. Our two methods offer potentially cost-effective strategies by leveraging errors to enhance reasoning capabilities, which costs significantly less than creating meticulously hand-crafted golden references. We ultimately make a thorough analysis of the reasons behind LLMs' errors, which provides directions that future research needs to overcome. </li>
                    </ul>
                </details>
              </li>
          </ul>
  
          <h4 class="title is-4">Paper Lists</h4>
          <ul type='disc'> 
            <li>
                <a href="https://github.com/Zhen-Tan-dmml/LLM4Annotation">LLMs for Data Annotation</a><br> A curated list of papers related to data annotation and synthesis with LLM
              </li>
          </ul>
        </div>
      </div>
    </div>
  </div>
</section>
  

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">People</h2>
        <div class="content has-text-justified">
          <div class="avatars">
            <div class="column">
              <center>
                
              <img class="imageo" src="./static/images/dawei.png"   align="middle">
              <br>
              <font size="3" ><a style="color:inherit;" href="https://david-li0406.github.io/">Dawei Li</a></font>
              </center>
            </div>

            <div class="column">
              <center>
                
              <img class="imageo" src="./static/images/shiping.jpg"   align="middle">
              <br>
              <font size="3" ><a style="color:inherit;" href="https://maybenotime.github.io/">Shiping Yang</a></font>
              </center>
            </div>
            
            <div class="column">
              <center>
                
              <img class="imageo" src="./static/images/hengyuan.png"   align="middle">
              <br>
              <font size="3" ><a style="color:inherit;" href="https://rattlesnakey.github.io/">Hengyuan Zhang</a></font>
              </center>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section> 


  
<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Acknowledgement</h2>
        <div class="content has-text-justified">
          <p>We sincerely thank the advisors and collaborators in our research work.</p>
          <!-- <p>We sincerely thank the people below for their guidance and collaboration in our research work. </p>
          <p> Advisory: <b><a style="color:inherit;" href="http://nshah.net/">Neil Shah</a></font></b>, <b><a style="color:inherit;" href="https://tzhao.io/">Tong Zhao</a></b>, <b><a style="color:inherit;" href="https://yaoma24.github.io/">Yao Ma</a></b>, <b><a style="color:inherit;" href="https://www.cs.emory.edu/~wjin30/">Wei Jin</a></b>, <b><a style="color:inherit;" href="https://www.linkedin.com/in/michael-galkin-80b71294">Michael Galkin</a></b>, <b><a style="color:inherit;" href="https://jian-tang.com/">Jian Tang</a></b>, <b><a style="color:inherit;" href="https://www.cs.ox.ac.uk/people/michael.bronstein/">Michael Bronstein</a></b>, <b><a style="color:inherit;" href="https://graphdeeplearning.github.io/authors/xavier-bresson/">Xavier Bresson</a></b>,
          <b><a style="color:inherit;" href="https://bhooi.github.io/">Bryan Hooi</a></b>,<b><a style="color:inherit;" href="https://www.amazon.science/author/haiyang-zhang">Haiyang Zhang</a></b>,<b><a style="color:inherit;" href="https://xta.ng/">Xiafeng Tang</a></b>,<b><a style="color:inherit;" href="http://chen-luo.com/">Chen Luo</a></b>.
          </p>

          <p>
            Students: <b><a style="color:inherit;" href="https://www.cse.msu.edu/~shomerha/">Harry Shomer</a></b>, <b><a style="color:inherit;" href="https://juanhui28.github.io/">Juanhui Li</a></b>, <b>Guangliang Liu</b>,<b><a style="color:inherit;" href="https://andyjzhao.github.io/">Jianan Zhao</a></b>, <b><a style="color:inherit;" href="https://xiaoxinhe.github.io/">Xiaoxin He</a></b>, <b><a style="color:inherit;" href="https://q-hwang.github.io/">Qian Huang</a></b>,
            <b><a style="color:inherit;" href="https://mila.quebec/en/person/xinyu-yuan/">Xinyu Yuan</a></b>, <b><a style="color:inherit;" href="https://kiddozhu.github.io/">Zhaocheng Zhu</a></b>.
          </p> -->
        

          
          
        </div>
      </div>
    </div>
  </div>
</section> 

  
  

<!-- <section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Sponsors</h2>
        <div class="content has-text-justified">
          <div class="sponsors">
            <div class="column">
              <img src="./static/images/JPMorgan.png" align="middle">
            </div>
            <div class="column">
              <img src="./static/images/Snap.png"  align="middle">
            </div>
            <div class="column">
              <img src="./static/images/Baidu.png" align="middle">
            </div>
            <div class="column">
              <img src="./static/images/Microsoft.png" align="middle">
            </div>
            <div class="column">
              <img src="./static/images/Cisco.png" align="middle">
            </div>
            <div class="column">
              <img src="./static/images/Amazon.png" align="middle">
            </div>
          </div>


          
        </div>
      </div>
    </div>
  </div>
</section>  -->





<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

  <script src="script.js"></script>
</body>
</html>