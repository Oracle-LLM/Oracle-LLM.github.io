<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="OracleLLM">
  <meta name="keywords" content="OracleLLM">
  <meta name="viewport" content="width=device-width, user-scalable=yes, initial-scale=0.3, maxmum-scale=1.0, minimum-scale=0.3">
  <style>
    body {
      display: flex;
      flex-direction: column;
    }

    
    .dot {
            height: 10px;
            width: 10px;
            background-color: black;
            border-radius: 50%;
            display: inline-block;
          }
    .column {
            float: left;
            width: 33.33%;
            padding: 5px;
          }
    .column2 {
        float: left;
        width: 48%;
        padding: 5px;
      }
.imageo{
  margin-top: 20px;
  width: 200px;
  height: 200px;
  border-radius:200px;
}
/* 清除图像容器后的浮动 */
.row::after {
  content: "";
  clear: both;
  display: table;
}
  </style>
  <title>OracleLLM: Empowering LLM with Self-Feedback</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">
  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <!-- (A) LOAD CSS & JS -->
  <link rel="stylesheet" href="collapse-list.css">
  <link rel="stylesheet" type="text/css" href="style.css">
  <script src="collapse-list.js"></script>


  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <img src="static/images/logo.png" alt="OracleLLM: Empowering LLM with Self-Feedback" width="800"/>
          <!-- <h1 class="title is-1 publication-title">OracleLLM: Empowering AI with<br>Self-Feedback</h1> -->
        </div>
      </div>
    </div>
  </div>
</section>



<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">About</h2>
        <div class="content has-text-justified">
          <p>
            <span>OracleLLM is an open-source research organization dedicated to exploring and advancing the concept of <strong>LLMs-as-Oracles</strong>. LLMs-as-Oracle refers to replacing human-generated golden references with the output and feedback produced by large language models (LLMs), treating them as authoritative benchmarks and learning signals for various tasks and applications. Our research areas include:</span>
            <div>(1). Data synthesis and annotation with LLMs.</div>
            <div>(2). LLMs as evaluators or judges for various tasks.</div>
            <div>(3). Employing LLMs' self-feedback as guidance for planning, self-improving and decision-making.</div>
            <span>Check more details on our current progress including papers, projects, and reading list!</span>
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Papers</h2>
        <div class="content has-text-justified">

          <h4 class="title is-4">Survey & Benchmark</h4>
          <ul type='disc'> 
            <li>
              <a href="https://arxiv.org/pdf/2402.13446">Large Language Models for Data Annotation: A Survey.</a><br> Zhen Tan*, Dawei Li*, Song Wang*, Alimohammad Beigi, Bohan Jiang, Amrita Bhattacharjee, Mansooreh Karami, Jundong Li, Lu Cheng, Huan Liu;<br><b>EMNLP 2024</b>
              <details>
                <summary> Details </summary>
                <ul type='disc'>
                      <li> Data annotation is the labeling or tagging of raw data with relevant information, essential for improving the efficacy of machine learning models. The process, however, is labor-intensive and expensive. The emergence of advanced Large Language Models (LLMs), exemplified by GPT-4, presents an unprecedented opportunity to revolutionize and automate the intricate process of data annotation. While existing surveys have extensively covered LLM architecture, training, and general applications, this paper uniquely focuses on their specific utility for data annotation. This survey contributes to three core aspects: LLM-Based Data Annotation, Assessing LLM-generated Annotations, and Learning with LLM-generated annotations. Furthermore, the paper includes an in-depth taxonomy of methodologies employing LLMs for data annotation, a comprehensive review of learning strategies for models incorporating LLM-generated annotations, and a detailed discussion on primary challenges and limitations associated with using LLMs for data annotation. As a key guide, this survey aims to direct researchers and practitioners in exploring the potential of the latest LLMs for data annotation, fostering future advancements in this critical domain. </li>
                </ul>
              </details>
            </li>

                <li>
                  <a href="">A Survey for LLM-as-a-Judge</a><br><b>Comming Soon!</b>
                </li>
          </ul>

          <h4 class="title is-4">Data Synthesis</h4>
          <ul type='disc'>
            <li>
              <a href="https://arxiv.org/pdf/2405.04819">DALK:Dynamic Co-Augmentation of LLMs and KG to Answer Alzheimer's Disease Questions with Scientific Literature.</a><br> Dawei Li, Shu Yang, Zhen Tan, Jae Young Baik, Sunkwon Yun, Joseph Lee, Aaron Chacko, Bojian Hou, Duy Duong-Tran, Ying Ding, Huan Liu, Li Shen, Tianlong Chen;<br><b>EMNLP 2024</b>
              <details>
                <summary> Details </summary>
                <ul type='disc'>
                      <li> Recent advancements in large language models (LLMs) have achieved promising performances across various applications. Nonetheless, the ongoing challenge of integrating long-tail knowledge continues to impede the seamless adoption of LLMs in specialized domains. In this work, we introduce DALK, a.k.a. Dynamic Co-Augmentation of LLMs and KG, to address this limitation and demonstrate its ability on studying Alzheimer's Disease (AD), a specialized sub-field in biomedicine and a global health priority. With a synergized framework of LLM and KG mutually enhancing each other, we first leverage LLM to construct an evolving AD-specific knowledge graph (KG) sourced from AD-related scientific literature, and then we utilize a coarse-to-fine sampling method with a novel self-aware knowledge retrieval approach to select appropriate knowledge from the KG to augment LLM inference capabilities. The experimental results, conducted on our constructed AD question answering (ADQA) benchmark, underscore the efficacy of DALK. Additionally, we perform a series of detailed analyses that can offer valuable insights and guidelines for the emerging topic of mutually enhancing KG and LLM. </li>
                </ul>
            </details>
            </li> 
            <li>
              <a href="https://arxiv.org/pdf/2402.01729">Contextualization Distillation from Large Language Model for Knowledge Graph Completion.</a><br>Dawei Li, Zhen Tan, Tianlong Chen, Huan Liu;<br><b>EACL 2024</b>
              <details>
                <summary> Details </summary>
                <ul type='disc'>
                      <li> While textual information significantly enhances the performance of pre-trained language models (PLMs) in knowledge graph completion (KGC), the static and noisy nature of existing corpora collected from Wikipedia articles or synsets definitions often limits the potential of PLM-based KGC models. To surmount these challenges, we introduce the Contextualization Distillation strategy, a versatile plug-in-and-play approach compatible with both discriminative and generative KGC frameworks. Our method begins by instructing large language models (LLMs) to transform compact, structural triplets into context-rich segments. Subsequently, we introduce two tailored auxiliary tasks, reconstruction and contextualization, allowing smaller KGC models to assimilate insights from these enriched triplets. Comprehensive evaluations across diverse datasets and KGC techniques highlight the efficacy and adaptability of our approach, revealing consistent performance enhancements irrespective of underlying pipelines or architectures. Moreover, our analysis makes our method more explainable and provides insight into generating path selection, as well as the choosing of suitable distillation tasks. </li>
                </ul>
            </details>
            </li>
          </ul>

          
          <h4 class="title is-4">LLM-as-a-Judge</h4>
          <ul type='disc'> 
            <li>
              <a href="https://arxiv.org/pdf/2405.04086">Optimizing Language Model's Reasoning Abilities with Weak Supervision</a><br>Yongqi Tong, Sizhe Wang, Dawei Li, Yifan Wang, Simeng Han, Zi Lin, Chengsong Huang, Jiaxin Huang, Jingbo Shang;<br><b>Arxiv Preprint</b>
              <details>
                <summary> Details </summary>
                <ul type='disc'>
                      <li> While Large Language Models (LLMs) have demonstrated proficiency in handling complex reasoning, much of the past work has depended on extensively annotated datasets by human experts. However, this reliance on fully-supervised annotations poses scalability challenges, particularly as models and data requirements grow. In this work, we begin by analyzing the limitations of existing data-efficient reinforcement learning (RL) methods in LLMs' reasoning enhancement. To mitigate this, we introduce self-reinforcement, an efficient weak-to-strong approach to optimize language models' reasoning abilities utilizing both annotated and unlabeled samples. Our method enhances the quality of synthetic feedback by fully harnessing annotated seed data and introducing a novel self-filtering mechanism to remove invalid pairs. We also present PuzzleBen, a weakly supervised benchmark for reasoning that comprises 25,147 complex questions, answers, and human-generated rationales across various domains, such as brainteasers, puzzles, riddles, parajumbles, and critical reasoning tasks. Our experiments underscore the significance of PuzzleBen, as well as the effectiveness of our methodology as a promising direction in future endeavors. </li>
                </ul>
            </details>
            </li>
            
            <li>
              <a href="https://arxiv.org/pdf/2407.04842">MJ-BENCH: Is Your Multimodal Reward Model Really a Good Judge for Text-to-Image Generation?</a><br>Zhaorun Chen, Yichao Du, Zichen Wen, Yiyang Zhou, Chenhang Cui, Zhenzhen Weng, Haoqin Tu, Chaoqi Wang, Zhengwei Tong, Qinglan Huang, Canyu Chen, Qinghao Ye, Zhihong Zhu, Yuqing Zhang, Jiawei Zhou, Zhuokai Zhao, Rafael Rafailov, Chelsea Finn, Huaxiu Yao;<br><b>Arxiv Preprint</b>
              <details>
                <summary> Details </summary>
                <ul type='disc'>
                      <li> While text-to-image models like DALLE-3 and Stable Diffusion are rapidly proliferating, they often encounter challenges such as hallucination, bias, and the production of unsafe, low-quality output. To effectively address these issues, it is crucial to align these models with desired behaviors based on feedback from a multimodal judge. Despite their significance, current multimodal judges frequently undergo inadequate evaluation of their capabilities and limitations, potentially leading to misalignment and unsafe fine-tuning outcomes. To address this issue, we introduce MJ-Bench, a novel benchmark which incorporates a comprehensive preference dataset to evaluate multimodal judges in providing feedback for image generation models across four key perspectives: alignment, safety, image quality, and bias. Specifically, we evaluate a large variety of multimodal judges including smaller-sized CLIP-based scoring models, open-source VLMs (e.g. LLaVA family), and close-source VLMs (e.g. GPT-4o, Claude 3) on each decomposed subcategory of our preference dataset. Experiments reveal that close-source VLMs generally provide better feedback, with GPT-4o outperforming other judges in average. Compared with open-source VLMs, smaller-sized scoring models can provide better feedback regarding text-image alignment and image quality, while VLMs provide more accurate feedback regarding safety and generation bias due to their stronger reasoning capabilities. Further studies in feedback scale reveal that VLM judges can generally provide more accurate and stable feedback in natural language (Likert-scale) than numerical scales. Notably, human evaluations on end-to-end fine-tuned models using separate feedback from these multimodal judges provide similar conclusions, further confirming the effectiveness of MJ-Bench. </li>
                </ul>
              </details>
            </li>
            
            <li>
              <a href="https://arxiv.org/pdf/2309.13788">Can LLM-Generated Misinformation Be Detected?</a><br>Canyu Chen, Kai Shu;<br><b>ICLR 2024</b>
              <details>
                <summary> Details </summary>
                <ul type='disc'>
                      <li> The advent of Large Language Models (LLMs) has made a transformative impact. However, the potential that LLMs such as ChatGPT can be exploited to generate misinformation has posed a serious concern to online safety and public trust. A fundamental research question is: will LLM-generated misinformation cause more harm than human-written misinformation? We propose to tackle this question from the perspective of detection difficulty. We first build a taxonomy of LLM-generated misinformation. Then we categorize and validate the potential real-world methods for generating misinformation with LLMs. Then, through extensive empirical investigation, we discover that LLM-generated misinformation can be harder to detect for humans and detectors compared to human-written misinformation with the same semantics, which suggests it can have more deceptive styles and potentially cause more harm. We also discuss the implications of our discovery on combating misinformation in the age of LLMs and the countermeasures. </li>
                </ul>
              </details>
            </li>

            <li>
              <a href="https://arxiv.org/pdf/2403.12403">Towards Interpretable Hate Speech Detection using Large Language Model-extracted Rationales</a><br>Ayushi Nirmal, Amrita Bhattacharjee, Paras Sheth, Huan Liu;<br><b>NAACL WOAH 2024</b>
              <details>
                <summary> Details </summary>
                <ul type='disc'>
                      <li> Although social media platforms are a prominent arena for users to engage in interpersonal discussions and express opinions, the facade and anonymity offered by social media may allow users to spew hate speech and offensive content. Given the massive scale of such platforms, there arises a need to automatically identify and flag instances of hate speech. Although several hate speech detection methods exist, most of these black-box methods are not interpretable or explainable by design. To address the lack of interpretability, in this paper, we propose to use state-of-the-art Large Language Models (LLMs) to extract features in the form of rationales from the input text, to train a base hate speech classifier, thereby enabling faithful interpretability by design. Our framework effectively combines the textual understanding capabilities of LLMs and the discriminative power of state-of-the-art hate speech classifiers to make these classifiers faithfully interpretable. Our comprehensive evaluation on a variety of English language social media hate speech datasets demonstrate: (1) the goodness of the LLM-extracted rationales, and (2) the surprising retention of detector performance even after training to ensure interpretability. </li>
                </ul>
              </details>
            </li>

            <li>
              <a href="https://arxiv.org/pdf/2308.01284">Fighting Fire with Fire: Can ChatGPT Detect AI-generated Text?</a><br>Amrita Bhattacharjee, Huan Liu;<br> <b>SIGKDD Explorations</b>
              <details>
                  <summary> Details </summary>
                  <ul type='disc'>
                        <li> Large language models (LLMs) such as ChatGPT are increasingly being used for various use cases, including text content generation at scale. Although detection methods for such AI-generated text exist already, we investigate ChatGPT's performance as a detector on such AI-generated text, inspired by works that use ChatGPT as a data labeler or annotator. We evaluate the zero-shot performance of ChatGPT in the task of human-written vs. AI-generated text detection, and perform experiments on publicly available datasets. We empirically investigate if ChatGPT is symmetrically effective in detecting AI-generated or human-written text. Our findings provide insight on how ChatGPT and similar LLMs may be leveraged in automated detection pipelines by simply focusing on solving a specific aspect of the problem and deriving the rest from that solution. </li>
                  </ul>
              </details>
            </li>
           </ul>

           <li>
            <a href="https://arxiv.org/pdf/2308.01284">SMoA: Improving Multi-agent Large Language Models with Sparse Mixture-of-Agents</a><br>Dawei Li, Zhen Tan, Peijia Qian, Yifan Li, Kumar Satvik Chaudhary, Lijie Hu, Jiayi Shen;<br> <b>Arxiv</b>
            <details>
                <summary> Details </summary>
                <ul type='disc'>
                      <li> While multi-agent systems have been shown to significantly enhance the performance of Large Language Models (LLMs) across various tasks and applications, the dense interaction between scaling agents potentially hampers their efficiency and diversity. To address these challenges, we draw inspiration from the sparse mixture-of-agents (SMoE) and propose a sparse mixture-of-agents (SMoA) framework to improve the efficiency and diversity of multi-agent LLMs. Unlike completely connected structures, SMoA introduces novel Response Selection and Early Stopping mechanisms to sparsify information flows among individual LLM agents, striking a balance between performance and efficiency. Additionally, inspired by the expert diversity principle in SMoE frameworks for workload balance between experts, we assign distinct role descriptions to each LLM agent, fostering diverse and divergent thinking. Extensive experiments on reasoning, alignment, and fairness benchmarks demonstrate that SMoA achieves performance comparable to traditional mixture-of-agents approaches but with significantly lower computational costs. Further analysis reveals that SMoA is more stable, has a greater capacity to scale, and offers considerable potential through hyper-parameter optimization. </li>
                </ul>
            </details>
          </li>
         </ul>


           <h4 class="title is-4">Self-Feedback</h4>
          <ul type='disc'> 
            <li>
              <a href="https://arxiv.org/pdf/2403.20046">Can LLMs Learn from Previous Mistakes? Investigating LLMs' Errors to Boost for Reasoning</a><br> Yongqi Tong, Dawei Li, Sizhe Wang, Yujia Wang, Fei Teng, Jingbo Shang;<br> <b>ACL 2024</b>
              <details>
                  <summary> Details </summary>
                  <ul type='disc'>
                        <li> Recent works have shown the benefits to LLMs from fine-tuning golden-standard Chain-of-Thought (CoT) rationales or using them as correct examples in few-shot prompting. While humans can indeed imitate correct examples, learning from our mistakes is another vital aspect of human cognition. Hence, a question naturally arises: \textit{can LLMs learn and benefit from their mistakes, especially for their reasoning? } This study investigates this problem from both the prompting and model-tuning perspectives. We begin by introducing \textsc{CoTErrorSet}, a new benchmark with 609,432 questions, each designed with both correct and error references, and demonstrating the types and reasons for making such mistakes. To explore the effectiveness of those mistakes, we design two methods: (1) \textbf{Self-rethinking} prompting guides LLMs to rethink whether they have made similar previous mistakes; and (2) \textbf{Mistake tuning} involves finetuning models in both correct and incorrect reasoning domains, rather than only tuning models to learn ground truth in traditional methodology. We conduct a series of experiments to prove LLMs can obtain benefits from mistakes in both directions. Our two methods offer potentially cost-effective strategies by leveraging errors to enhance reasoning capabilities, which costs significantly less than creating meticulously hand-crafted golden references. We ultimately make a thorough analysis of the reasons behind LLMs' errors, which provides directions that future research needs to overcome. </li>
                  </ul>
              </details>
            </li>

            <li>
              <a href="https://arxiv.org/pdf/2309.13340">Towards LLM-guided Causal Explainability for Black-box Text Classifiers</a><br>Amrita Bhattacharjee, Raha Moraffah, Joshua Garland, Huan Liu;<br> <b>AAAI ReLM 2024</b>
              <details>
                  <summary> Details </summary>
                  <ul type='disc'>
                        <li> With the advent of larger and more complex deep learning models, such as in Natural Language Processing (NLP), model qualities like explainability and interpretability, albeit highly desirable, are becoming harder challenges to tackle and solve. For example, state-of-the-art models in text classification are black-box by design. Although standard explanation methods provide some degree of explainability, these are mostly correlation-based methods and do not provide much insight into the model. The alternative of causal explainability is more desirable to achieve but extremely challenging in NLP due to a variety of reasons. Inspired by recent endeavors to utilize Large Language Models (LLMs) as experts, in this work, we aim to leverage the instruction-following and textual understanding capabilities of recent state-of-the-art LLMs to facilitate causal explainability via counterfactual explanation generation for black-box text classifiers. To do this, we propose a three-step pipeline via which, we use an off-the-shelf LLM to: (1) identify the latent or unobserved features in the input text, (2) identify the input features associated with the latent features, and finally (3) use the identified input features to generate a counterfactual explanation. We experiment with our pipeline on multiple NLP text classification datasets, with several recent LLMs, and present interesting and promising findings. </li>
                  </ul>
              </details>
            </li>

            <li>
              <a href="https://arxiv.org/pdf/2401.04925">The Impact of Reasoning Step Length on Large Language Models</a><br>Mingyu Jin*, Qinkai Yu*, Dong Shu, Haiyan Zhao, Wenyue Hua, Yanda Meng, Yongfeng Zhang, Mengnan Du;<br> <b>ACL 2024</b>
              <details>
                  <summary> Details </summary>
                  <ul type='disc'>
                        <li> Chain of Thought (CoT) is significant in improving the reasoning abilities of large language models (LLMs). However, the correlation between the effectiveness of CoT and the length of reasoning steps in prompts remains largely unknown. To shed light on this, we have conducted several empirical experiments to explore the relations. Specifically, we design experiments that expand and compress the rationale reasoning steps within CoT demonstrations while keeping all other factors constant. We have the following key findings. First, the results indicate that lengthening the reasoning steps in prompts, even without adding new information into the prompt, considerably enhances LLMs' reasoning abilities across multiple datasets. Alternatively, shortening the reasoning steps, even while preserving the key information, significantly diminishes the reasoning abilities of models. This finding highlights the importance of the number of steps in CoT prompts and provides practical guidance to make better use of LLMs' potential in complex problem-solving scenarios. Second, we also investigated the relationship between the performance of CoT and the rationales used in demonstrations. Surprisingly, the result shows that even incorrect rationales can yield favorable outcomes if they maintain the requisite length of inference. Third, we observed that the advantages of increasing reasoning steps are task-dependent: simpler tasks require fewer steps, whereas complex tasks gain significantly from longer inference sequences. </li>
                  </ul>
              </details>
            </li>

          </ul>
  
          <h4 class="title is-4">Paper Lists</h4>
          <ul type='disc'> 
            <li>
                <a href="https://github.com/Zhen-Tan-dmml/LLM4Annotation">LLMs for Data Annotation</a><br> A curated list of papers related to data annotation and synthesis with LLM
              </li>
          </ul>
        </div>
      </div>
    </div>
  </div>
</section>
  

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">People</h2>
        <div class="content has-text-justified">
          <div class="avatars">
            <div class="column">
              <center>
                
              <img class="imageo" src="./static/images/dawei.png"   align="middle">
              <br>
              <font size="3" ><a style="color:inherit;" href="https://david-li0406.github.io/">Dawei Li</a></font>
              </center>
            </div>

            <div class="column">
              <center>
                
              <img class="imageo" src="./static/images/shiping.jpg"   align="middle">
              <br>
              <font size="3" ><a style="color:inherit;" href="https://maybenotime.github.io/">Shiping Yang</a></font>
              </center>
            </div>
            
            <div class="column">
              <center>
                
              <img class="imageo" src="./static/images/hengyuan.png"   align="middle">
              <br>
              <font size="3" ><a style="color:inherit;" href="https://rattlesnakey.github.io/">Hengyuan Zhang</a></font>
              </center>
            </div>

            <div class="column">
              <center>
                
              <img class="imageo" src="./static/images/canyu.jpg"   align="middle">
              <br>
              <font size="3" ><a style="color:inherit;" href="https://canyuchen.com/">Canyu Chen</a></font>
              </center>
            </div>

            <div class="column">
              <center>
                
              <img class="imageo" src="./static/images/amrita.jpg"   align="middle">
              <br>
              <font size="3" ><a style="color:inherit;" href="https://amritabh.github.io/">Amrita Bhattacharjee</a></font>
              </center>
            </div>

            <div class="column">
              <center>
                
              <img class="imageo" src="./static/images/mingyu.jpg"   align="middle">
              <br>
              <font size="3" ><a style="color:inherit;" href="https://mingyuj666.github.io/">Mingyu Jin</a></font>
              </center>
            </div>

            <div class="column">
              <center>
                
              <img class="imageo" src="./static/images/Haolun.jpg"   align="middle">
              <br>
              <font size="3" ><a style="color:inherit;" href="https://haolun-wu.github.io/">Haolun Wu</a></font>
              </center>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section> 


  
<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Acknowledgement</h2>
        <div class="content has-text-justified">
          <p>We sincerely thank the advisors and collaborators in our research work.</p>
          <!-- <p>We sincerely thank the people below for their guidance and collaboration in our research work. </p>
          <p> Advisory: <b><a style="color:inherit;" href="http://nshah.net/">Neil Shah</a></font></b>, <b><a style="color:inherit;" href="https://tzhao.io/">Tong Zhao</a></b>, <b><a style="color:inherit;" href="https://yaoma24.github.io/">Yao Ma</a></b>, <b><a style="color:inherit;" href="https://www.cs.emory.edu/~wjin30/">Wei Jin</a></b>, <b><a style="color:inherit;" href="https://www.linkedin.com/in/michael-galkin-80b71294">Michael Galkin</a></b>, <b><a style="color:inherit;" href="https://jian-tang.com/">Jian Tang</a></b>, <b><a style="color:inherit;" href="https://www.cs.ox.ac.uk/people/michael.bronstein/">Michael Bronstein</a></b>, <b><a style="color:inherit;" href="https://graphdeeplearning.github.io/authors/xavier-bresson/">Xavier Bresson</a></b>,
          <b><a style="color:inherit;" href="https://bhooi.github.io/">Bryan Hooi</a></b>,<b><a style="color:inherit;" href="https://www.amazon.science/author/haiyang-zhang">Haiyang Zhang</a></b>,<b><a style="color:inherit;" href="https://xta.ng/">Xiafeng Tang</a></b>,<b><a style="color:inherit;" href="http://chen-luo.com/">Chen Luo</a></b>.
          </p>

          <p>
            Students: <b><a style="color:inherit;" href="https://www.cse.msu.edu/~shomerha/">Harry Shomer</a></b>, <b><a style="color:inherit;" href="https://juanhui28.github.io/">Juanhui Li</a></b>, <b>Guangliang Liu</b>,<b><a style="color:inherit;" href="https://andyjzhao.github.io/">Jianan Zhao</a></b>, <b><a style="color:inherit;" href="https://xiaoxinhe.github.io/">Xiaoxin He</a></b>, <b><a style="color:inherit;" href="https://q-hwang.github.io/">Qian Huang</a></b>,
            <b><a style="color:inherit;" href="https://mila.quebec/en/person/xinyu-yuan/">Xinyu Yuan</a></b>, <b><a style="color:inherit;" href="https://kiddozhu.github.io/">Zhaocheng Zhu</a></b>.
          </p> -->
        

          
          
        </div>
      </div>
    </div>
  </div>
</section> 

  
  

<!-- <section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Sponsors</h2>
        <div class="content has-text-justified">
          <div class="sponsors">
            <div class="column">
              <img src="./static/images/JPMorgan.png" align="middle">
            </div>
            <div class="column">
              <img src="./static/images/Snap.png"  align="middle">
            </div>
            <div class="column">
              <img src="./static/images/Baidu.png" align="middle">
            </div>
            <div class="column">
              <img src="./static/images/Microsoft.png" align="middle">
            </div>
            <div class="column">
              <img src="./static/images/Cisco.png" align="middle">
            </div>
            <div class="column">
              <img src="./static/images/Amazon.png" align="middle">
            </div>
          </div>


          
        </div>
      </div>
    </div>
  </div>
</section>  -->





<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

  <script src="script.js"></script>
</body>
</html>