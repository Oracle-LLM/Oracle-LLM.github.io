<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Deformable Neural Radiance Fields creates free-viewpoint portraits (nerfies) from casually captured videos.">
  <meta name="keywords" content="Nerfies, D-NeRF, NeRF">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <style>
    body {
      display: flex;
      flex-direction: column;
    }

    
    .dot {
            height: 10px;
            width: 10px;
            background-color: black;
            border-radius: 50%;
            display: inline-block;
          }
    .column {
            float: left;
            width: 33.33%;
            padding: 5px;
          }
    .column2 {
        float: left;
        width: 48%;
        padding: 5px;
      }
    .imageo{
      margin-top: 20px;
      width: 200px;
      height: 200px;
      border-radius:200px;
    }
    /* 清除图像容器后的浮动 */
    .row::after {
      content: "";
      clear: both;
      display: table;
    }
  </style>
  
  <title>OracleLLM</title>


  <section class="section">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Introduction</h2>
          <div class="content has-text-justified">
            <p>
              Greetings! We are a tiny research group from the <a href="https://dse.cse.msu.edu/">Data Science and Engineering (DSE) Laboratory</a> at Michigan State University.
               We typically focus on the Graph Foundation Models (GFM). Our perspectives are as follows: (1) LLM can be one choice for building GFM, but not yet. (2) GFM requires the guidance from theoretical principles. This is exciting as it connects the advanced progress from theory to unbeatable empirical success (<a href="https://arxiv.org/abs/2402.02216">Check details here</a>). (3) There is an initial spark for Neural scaling law on graphs. We need more high-quality data, a better model backbone, and a better pre-training task design toward scaling. (4) the most important thing for GFM is the correct application scenario. Beyond traditional graph topics in the data mining domain, we are also interested in the potential of the utilization of GFM in other domains. Check more details on our current progress including papers, talks, open-source repository, and reading list.
            </p>
  
          </div>
        </div>
      </div>
    </div>
  </section>


  <section class="section">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Papers</h2>
          <div class="content has-text-justified">
  
            <h4 class="title is-4">Prospective</h4>
            <ul type='disc'> 
              <li>
                <a href="https://arxiv.org/abs/2402.02216">Graph Foundation Models</a><br> Haitao Mao*, Zhikai Chen*, Wenzhuo Tang, Jianan Zhao, Yao Ma, Tong Zhao, Neil Shah, Michael Galkin, Jiliang Tang;<br><b>ICML 2024 Spotlight</b>
                <details >
                    <summary>Details</summary>
                    <ul type='disc'>
                      <li>We propose a “graph vocabulary” perspective aiming to find the basic transferable units underlying graphs that encode the invariance of graphs</li>
                      <li>We illustrate theoretical guidance for the graph vocabulary design</li>
                      <li>We emphasize the practical techniques for building GFM following the Neural Scaling Law</li>
                    </ul>
                  </details>
                  </li>
            </ul>
  
            <h4 class="title is-4">GFMs</h4>
            <ul type='disc'> 
              <li>
                <a href="">A Pure Transformer Pretraining Framework on Text-attributed Graphs.</a><br> Yu Song, Haitao Mao, Jiachen Xiao, Jingzhe Liu, Zhikai Chen, Wei Jin, Carl Yang, Jiliang Tang, Hui Liu;<br><b>preprint, 2024</b>
              </li>
              <li>
                <a href="https://arxiv.org/pdf/2406.01899">Cross-Domain Graph Data Scaling: A Showcase with Diffusion Models.</a><br> Wenzhuo Tang, Haitao Mao, Danial Dervovic, Ivan Brugere, Saumitra Mishra, Yuying Xie, Jiliang Tang.;<br><b>preprint, 2024</b>
              </li>
              <li>
                <a href="https://arxiv.org/pdf/2402.07738">Universal link predictor by in-context learning.</a><br> Kaiwen Dong, Haitao Mao, Zhichun Guo, Nitesh Chewla;<br><b>preprint, 2024</b>
              </li>
            </ul>
  
            
            <h4 class="title is-4">Principles</h4>
            <ul type='disc'>
              <li>
                  <a href="">Do Neural Scaling Laws exist on Graph Self-Supervised Learning</a><br> Qian Ma, Haitao Mao, Zhehua Zhang, Chunlin Feng, Jingzhe Liu, Yu Song, Yao Ma;<br><b>preprint, 2024</b>
                </li>  
              <li>
                  <a href="https://arxiv.org/abs/2402.02054">Neural Scaling Laws on Graphs</a><br> Jingzhe Liu, Haitao Mao, Zhikai Chen, Tong Zhao, Neil Shah, Jiliang Tang;<br><b>preprint, 2024</b>
                  <details>
                    <summary>Details</summary>
                    <ul type='disc'>
                      <li>We examine the mode and data scaling laws on graphs.</li>
                      <li>For model scaling, we observe some graph-specific phenomena and identify the potential reasons.</li>
                      <li>For data scaling, we propose that the total edge number is a better metric, and extend the data scaling law to node classification and link prediction tasks.</li>
                    </ul>
                  </details>
                </li>
                <li>
                  <a href="ttps://arxiv.org/abs/2402.02212">A Data Generation Perspective to the Mechanism of In-Context Learning</a><br> Haitao Mao, Guangliang Liu, Yao Ma, Rongrong Wang, Jiliang Tang;<br><b>preprint, 2024</b>
                <details>
                    <summary>Details</summary>
                    <ul type='disc'>
                      <li>We study the underlying mechanism of ICL from a data generation perspective.</li>
                      <li>we rigorously adopt the terms of skill learning and skill recognition. The difference between them is skill learning can learn new data generation functions from in-context data. </li>
                      <li>We illustrate two analysis frameworks, i.e., Bayesian inference statistical framework and function learning statistical framework.</li>
                    </ul>
                  </details>
                </li>
                <li>
                  <a href="https://arxiv.org/pdf/2310.00793">Revisiting Link Prediction: A data perspective</a><br> Haitao Mao, Juanhui Li, Harry Shomer, Bingheng Li, Wenqi Fan, Yao Ma, Tong Zhao, Neil Shah, Jiliang Tang;<br><b>ICLR, 2024</b>
                <details >
                    <summary>Details</summary>
                    <ul type='disc'>
                      <li>We recognize three fundamental factors critical to link prediction: local structural proximity, global structural proximity and feature proximity.</li>
                      <li>We unearth the incompatibility between feature and structural proximity.</li>
                      <li>We collect diverse link prediction datasets and provide new guidance for model architecture design. </li>
                    </ul>
                  </details>
                </li>
                <li>
                  <a href="https://arxiv.org/abs/2306.01323">Demystifying Structural Disparity in Graph Neural Networks: Can One Size Fit All?</a><br>Haitao Mao, Zhikai Chen, Wei Jin, Haoyu Han, Yao Ma, Tong Zhao, Neil Shah, Jiliang Tang; <b>NeurIPS, 2023</b>
                <details >
                    <summary>Details</summary>
                    <ul type='disc'>
                      <li>We recognize two fundamental factors critical to node classification: homophily and heterophily.</li>
                      <li>GNNs can only work on either the homophily pattern or the heterophily one, but not both.</li>
                    </ul>
                  </details>
                </li>
  
              </ul>
             <h4 class="title is-4">LLMs on Graphs</h4>
            <ul type='disc'> 
                <li>
                  <a href="https://arxiv.org/abs/2310.04668">Label-free Node Classification on Graphs with Large Language Models (LLMS)</a><br> Zhikai Chen, Haitao Mao, Hongzhi Wen, Haoyu Han, Wei Jin, Haiyang Zhang, Hui Liu, Jiliang Tang;<br> <b>ICLR, 2024</b>
                  <details>
                      <summary> Details </summary>
                      <ul type='disc'>
                            <li> In recent years, there have been remarkable advancements in node classification achieved by Graph Neural Networks (GNNs). However, they necessitate abundant high-quality labels to ensure promising performance. In contrast, Large Language Models (LLMs) exhibit impressive zero-shot proficiency on text-attributed graphs. Yet, they face challenges in efficiently processing structural data and suffer from high inference costs. In light of these observations, this work introduces a label-free node classification on graphs with LLMs pipeline, LLM-GNN. It amalgamates the strengths of both GNNs and LLMs while mitigating their limitations. </li>
                      </ul>
                  </details>
                </li>
                <li>
                  <a href="https://arxiv.org/abs/2307.03393v3">Exploring the Potential of Large Language Models (LLMs) in Learning on Graphs</a><br>Zhikai Chen, Haitao Mao, Hang Li, Wei Jin, Hongzhi Wen, Xiaochi Wei, Shuaiqiang Wang, Dawei Yin, Wenqi Fan, Hui Liu, Jiliang Tang;<br><b>SIGKDD Explorations and NeurIPS GLFrontiers 2023</b> [<a href="https://github.com/CurryTang/Graph-LLM">codes</a>]
                  <details >
                      <summary> Details </summary>
                      <ul type='disc'>
                          <li> In this paper, we study how LLMs can be used to empower graph machine learning problems. For node classification tasks, we propose two pipelines: <strong>LLMs-as-Enhancers</strong> and <strong>LLMs-as-Predictors</strong>. <strong>LLMs-as-Enhancers</strong> adopts LLMs to enhance the text features, which improves GNNs' performance. <strong>LLMs-as-Predictors</strong> directly adopts LLMs for inference, and present feature information together with inductive biases by natural languages. <strong>LLMs-as-Predictors</strong> achieves promising zero-shot performance. </li>
                      </ul>
                  </details>
                </li>
                <li>
                  <a href="https://arxiv.org/abs/2307.03393v3">Graph Machine Learning in the Era of Large Language Models (LLMs).</a><br>Wenqi Fan, Shijie Wang, Jiani Huang, Zhikai Chen, Yu Song, Wenzhuo Tang, Haitao Mao, Hui Liu, Xiaorui Liu, Dawei Yin, Qing Li;<br><b>Arxiv 2024</b>
                </li>
            </ul>
            <h4 class="title is-4">Benchmarks</h4>
            <ul type='disc'> 
              <li>
                  <a href="https://arxiv.org/pdf/2306.10453/">Evaluating Graph Neural Networks for Link Prediction: Current Pitfalls and New Benchmarking</a><br> Juanhui Li, Harry Shomer, Haitao Mao, Shenglai Zeng, Yao Ma, Neil Shah, Jiliang Tang, Dawei Yin;<br> <b>NeurIPS Dataset Track, 2023</b>
                </li>
              <li>
                  <a href="">Text-space graph foundation models: a comprehensive benchmark and new insights.</a><br> Zhikai Chen, Haitao Mao, Jingzhe Liu, Yu Song, Bingheng Li, Wei Jin, Bahare Fatemi, Anton Tsitsulin, Bryan Perozzi, Hui Liu, Jiliang Tang;<br> <b>Preprint 2024</b>
                </li>
            </ul>
            <h4 class="title is-4">Others</h4>
            <ul type='disc'> 
              <li>
                  <a href="https://arxiv.org/pdf/2112.00955.pdf">Source Free Graph Unsupervised Domain Adaptation</a><br> Haitao Mao, Lun Du, Yujia Zheng, Qiang Fu, Zelin Li, Xu Chen, Shi Han, Dongmei Zhang;<br> <b>WSDM, 2024</b>
                </li>
                <li>
                  <a href="https://arxiv.org/pdf/2310.11009.pdf">LPFormer: An Adaptive Graph Transformer for Link Prediction</a><br> Harry Shomer, Yao Ma, Haitao Mao, Juanhui Li, Bo Wu, Jiliang Tang;<br><b>preprint, 2023</b>
                </li>
            </ul>
            <h4 class="title is-4">Paper Lists</h4>
            <ul type='disc'> 
              <li>
                  <a href="https://github.com/CurryTang/Towards-Graph-Foundation-Models-New-perspective-">Towards graph foundation models: from the PRINCIPLE perspective</a><br> A curated list of papers related to the principles for graph foundation models
                </li>
                <li>
                  <a href="https://github.com/CurryTang/Towards-graph-foundation-models">Towards graph foundation models: from the METHOD perspective</a><br> A curated list of papers related to the practical techniques for graph foundation models
                </li>
            </ul>
          </div>
        </div>
      </div>
    </div>
  </section>



  <section class="section">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">People</h2>
          <div class="content has-text-justified">
            <div class="avatars">
              <div class="column">
                <center>
                  
                <img class="imageo" src="./static/images/Mao.png"   align="middle">
                <br>
                <font size="3" ><a style="color:inherit;" href="https://haitaomao.github.io/publications/">Haitao Mao</a></font>
                </center>
              </div>
  
              <div class="column">
                <center>
                  
                <img class="imageo" src="./static/images/Chen.png"   align="middle">
                <br>
                <font size="3" ><a style="color:inherit;" href="https://currytang.github.io/">Zhikai Chen</a></font>
                </center>
              </div>
  
         
              <div class="column">
                <center>
                <img class="imageo" src="./static/images/Tang.png"   align="middle">
                <br>
                <font size="3" ><a style="color:inherit;" href="https://wenzhuotang.github.io/">Wenzhuo Tang</a></font>
                </center>
              </div>
              
              <div class="column">
                <center>
                  
                <img class="imageo" src="./static/images/Song.png"   align="middle">
                <br>
                <font size="3" ><a style="color:inherit;" href="https://github.com/SongYYYY">Yu Song</a></font>
                </center>
              </div>
  
              <div class="column">
                <center>
                  
                <img class="imageo" src="./static/images/Liu.png"   align="middle">
                <br>
                <font size="3" ><a style="color:inherit;" href="https://liu-jingzhe.github.io/personalpage/">Jingzhe Liu</a></font>
                </center>
              </div>
  
      
              <div class="column">
                <center>
                <img class="imageo" src="./static/images/Dai.png"   align="middle">
                <br>
                <font size="3" ><a style="color:inherit;" href="https://ddigimon.github.io/">Xinnan Dai</a></font>
                </center>
              </div>  
            </div>
          </div>
        </div>
      </div>
    </div>
  </section> 


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
